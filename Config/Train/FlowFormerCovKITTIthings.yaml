Model:
  name: "FlowFormerCovKITTIThings"
  stage: "tartanair"
  suffix: "sintel"
  gamma: 0.85
  max_flow: 400
  batch_size: 6
  sum_freq: 100
  val_freq: 5000000
  image_size: [480, 640]
  add_noise: True
  critical_params: []
  transformer: "latentcostformer"
  restore_ckpt: "Model/things_kitti.pth"

  # latentcostformer
  latentcostformer:
    pe: "linear"
    dropout: 0.0
    encoder_latent_dim: 256 # in twins, this is 256
    query_latent_dim: 64
    cost_latent_input_dim: 64
    cost_latent_token_num: 8
    cost_latent_dim: 128
    arc_type: "transformer"
    cost_heads_num: 1
    # encoder
    pretrain: True
    context_concat: False
    encoder_depth: 3
    feat_cross_attn: False
    patch_size: 8
    patch_embed: "single"
    no_pe: False
    gma: "GMA"
    kernel_size: 9
    rm_res: True
    vert_c_dim: 64
    cost_encoder_res: True
    cnet: "twins"
    fnet: "twins"
    no_sc: False
    only_global: False
    add_flow_token: True
    use_mlp: False
    vertical_conv: False

    # decoder
    decoder_depth: 12
    critical_params: ["cost_heads_num", 
                      "vert_c_dim", 
                      "cnet", 
                      "pretrain", 
                      "add_flow_token", 
                      "encoder_depth", 
                      "gma", 
                      "cost_encoder_res"
                      ]

  ### TRAINER

  scheduler: "OneCycleLR"
  optimizer: "adamw"
  canonical_lr: 12.5e-5
  adamw_decay: 1.0e-5
  clip: 1.0
  num_steps: 120000
  epsilon: 1.0e-8
  anneal_strategy: "linear"

  folderlength: None
  autosave_freq: 5000
  num_workers: 0
  mixed_precision: True

  seed: 1234
  log_freq: 100
  datatype: fp32

Train:
  data: !flatten_seq 
    - !include ../Sequence/Training_Dataset/TartanAir_Train.yaml
    - !include ../Sequence/Training_Dataset/TartanAir_TrainHard.yaml

Evaluate:
  data: !flatten_seq 
    - !include ../Sequence/Training_Dataset/TartanAirV2_Test.yaml
